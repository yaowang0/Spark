之前的Standalone模式关闭：
./sbin/stop-all.sh

一、Spark YARN模式：
client模式 和 cluster模式

二、配置
1. 修改spark-env.sh文件
export HADOOP_CONF_DIR=$HADOOP_INSTALL/etc/hadoop              #Hadoop的配置文件所在的路径
export YARN_CONF_DIR=$HADOOP_INSTALL/etc/hadoop                #Hadoop的配置文件所在的路径
export SPARK_HOME=/usr/hadoopsoft/spark-1.3.1-bin-hadoop2.4    #Spark的配置文件所在的路径，这个路径是Hadoop来找Spark
export SPARK_JAR=/usr/hadoopsoft/spark-1.3.1-bin-hadoop2.4/lib/spark-assembly-1.3.1-hadoop2.4.0.jar
export PATH=$SPARK_HOME/bin:$PATH

2. ~/.bash/profile
配置好Hadoop的环境变量(Spark需要去找YARN)

三、提交
1. client模式(结果在XShell可见)：
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-client --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100

2. cluster模式(结果在YARN的8088的UI页面可见)：
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master yarn-cluster --executor-memory 1G --num-executors 1 ./lib/spark-examples-1.3.1-hadoop2.4.0.jar 100

